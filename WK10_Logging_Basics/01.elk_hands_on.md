# ELK Hands-on

## Step 1. Clone the repo and read the ELK repo 

Let us get a docker-elk
```
git clone https://github.com/deviantony/docker-elk
```

Quickly read through the repo and identify logstash.conf

## Step 2. Spin up the ELK stack

Now, let us spin up the ELK stack by
```
cd docker-elk
docker-compose up
```
Expecting to see something like this:
```
...
Creating docker-elk_elasticsearch_1 ... done
Creating docker-elk_kibana_1        ... done
Creating docker-elk_logstash_1      ... done
Attaching to docker-elk_elasticsearch_1, docker-elk_logstash_1, docker-elk_kibana_1
logstash_1       | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
elasticsearch_1  | Created elasticsearch keystore in /usr/share/elasticsearch/config/elasticsearch.keystore
kibana_1         | {"type":"log","@timestamp":"2020-07-20T11:03:35Z","tags":["warning","plugins-discovery"],"pid":7,"message":"Expect plugin \"id\" in camelCase, but found: apm_oss"}
```
The stack is pre-configured with the following privileged bootstrap user:
```
user: elastic
password: changeme
```
## Step 3. Change the password
Execute the following three commands in `docker-elk`

```
docker-compose exec elasticsearch bin/elasticsearch-reset-password --batch --user elastic
docker-compose exec elasticsearch bin/elasticsearch-reset-password --batch --user logstash_internal
docker-compose exec elasticsearch bin/elasticsearch-reset-password --batch --user kibana_system
```

Sample output will look like this

```
❯ docker-compose exec elasticsearch bin/elasticsearch-reset-password --batch --user elastic
Password for the [elastic] user successfully reset.
New value: Wb7Yv6niUXEayOtNMCl*
❯ docker-compose exec elasticsearch bin/elasticsearch-reset-password --batch --user logstash_internal
Password for the [logstash_internal] user successfully reset.
New value: 3cHnndylWE0Dm3eHNx6N
❯ docker-compose exec elasticsearch bin/elasticsearch-reset-password --batch --user kibana_system
Password for the [kibana_system] user successfully reset.
New value: DzA=zWV8dxvTqsm=MVNS
```

Now, update `.env` in docker-elk

```
ELASTIC_VERSION=8.2.3

## Passwords for stack users
#

# User 'elastic' (built-in)
#
# Superuser role, full access to cluster management and data indices.
# https://www.elastic.co/guide/en/elasticsearch/reference/current/built-in-users.html
ELASTIC_PASSWORD='Wb7Yv6niUXEayOtNMCl*'

# User 'logstash_internal' (custom)
#
# The user Logstash uses to connect and send data to Elasticsearch.
# https://www.elastic.co/guide/en/logstash/current/ls-security.html
LOGSTASH_INTERNAL_PASSWORD='3cHnndylWE0Dm3eHNx6N'

# User 'kibana_system' (built-in)
#
# The user Kibana uses to connect and communicate with Elasticsearch.
# https://www.elastic.co/guide/en/elasticsearch/reference/current/built-in-users.html
KIBANA_SYSTEM_PASSWORD='DzA=zWV8dxvTqsm=MVNS'
```
and then restart your service
```
docker-compose down
docker-compose up
```

## Step 4. Check the docker containers
Check the containers and logs, make sure they are running
```
docker ps
CONTAINER ID   IMAGE                      COMMAND                  CREATED         STATUS         PORTS                                                                                            NAMES
7054d9fafeb9   docker-elk_logstash        "/usr/local/bin/dock…"   7 minutes ago   Up 7 minutes   0.0.0.0:5000->5000/tcp, 0.0.0.0:5044->5044/tcp, 0.0.0.0:9600->9600/tcp, 0.0.0.0:5000->5000/udp   docker-elk_logstash_1
d506ff044997   docker-elk_kibana          "/bin/tini -- /usr/l…"   7 minutes ago   Up 7 minutes   0.0.0.0:5601->5601/tcp                                                                           docker-elk_kibana_1
2dba2aed0b46   docker-elk_elasticsearch   "/bin/tini -- /usr/l…"   7 minutes ago   Up 7 minutes   0.0.0.0:9200->9200/tcp, 0.0.0.0:9300->9300/tcp                                                   docker-elk_elasticsearch_1
```
ELK stack - FILEBEAT (Installed on your webapp servers) + Logstash + Elastic Search + Kibana

![Alt text](images/high_level.png?raw=true)

* **Filebeat** is a lightweight shipper for forwarding and centralizing log data. Installed as an agent on your servers,
  Filebeat monitors the log files or locations that you specify, collects log events, and forwards them either to 
  Elasticsearch or Logstash for indexing.
  
* **Logstash** is a free and open server-side data processing pipeline that ingests data from a multitude of sources, 
  transforms it, and then sends it to your favorite "stash." 
  
* **Elasticsearch** is a distributed, free and open search and analytics engine for all types of data, including textual, 
  numerical, geospatial, structured, and unstructured. Elasticsearch is built on Apache Lucene and was first released 
  in 2010 by Elasticsearch N.V. (now known as Elastic). Known for its simple REST APIs, distributed nature, speed, and
  scalability, Elasticsearch is the central component of the Elastic Stack, a set of free and open tools for data 
  ingestion, enrichment, storage, analysis, and visualization.
  
* **Kibana** is a free and open frontend application that sits on top of the Elastic Stack, providing search and data 
  visualization capabilities for data indexed in Elasticsearch. Commonly known as the charting tool for the Elastic
  Stack (previously referred to as the ELK Stack after Elasticsearch, Logstash, and Kibana), Kibana also acts as the
  user interface for monitoring, managing, and securing an Elastic Stack cluster — as well as the centralized hub for built-in solutions developed on the Elastic Stack.

## Step 3. Load the test/sample data
### 1. The UI way:
Login to localhost:5601 -> Home (http://localhost:5601/app/kibana#/home) -> Add Sample data -> Sample web logs -> Add data
![Alt text](images/add_sample_data.png?raw=true)

Similarly, if you would like to load your own data via the UI, I would recommend to get a syslog data or generate your
own application log (See WK4 How we generate logfile in python)

* Syslog location
```
/var/log/syslog
```
Windows may use the event logs, which is under:
```
C:\Windows\System32\winevt\Logs
```
But I have not yet verified.

Login to localhost:5601 -> Home (http://localhost:5601/app/kibana#/home) -> Import a CSV, NDJSON, or log file
-> Drag and drop your syslog file -> give index a prefix "logstash-*" and select @timestamp -> click through

### 2. The CLI way:
You can go ahead and inject some log entries. The shipped Logstash configuration allows you to send content via TCP:
```
# Using GNU netcat (CentOS, Fedora, MacOS Homebrew, ...)
$ cat /path/to/logfile.log | nc -c localhost 5000
```

```
# Using BSD netcat (Debian, Ubuntu, MacOS system, ...)
$ cat /path/to/logfile.log | nc -q0 localhost 5000
```

What is `nc`? https://linux.die.net/man/1/nc
Open another terminal and Create an index pattern via the Kibana API:
```
$ curl -XPOST -D- 'http://localhost:5601/api/saved_objects/index-pattern' \
    -H 'Content-Type: application/json' \
    -H 'kbn-version: 7.8.0' \
    -u elastic:<your generated elastic password> \
    -d '{"attributes":{"title":"logstash-*","timeFieldName":"@timestamp"}}'
```
or you can login to the UI -> "Connect to your Elasticsearch index" -> give index a prefix "logstash-*" and select @timestamp

## Step 3 View the data
Click http://localhost:5601/app/kibana#/discover
![Alt text](images/discover_the_data.png?raw=true)

and you should see sample logs or logstash indexes

Please click through any button on the page to get familiar with it.



## Step 4 Analyse the data
Let us go to dashboards http://localhost:5601/app/kibana#/dashboard
-> Create New -> Lens

What is Lens?
Kibana Lens is an easy-to-use, intuitive UI that simplifies the process of data visualization through a drag-and-drop 
experience. Whether you're exploring billions of logs or spotting trends from your website traffic, Lens gets you from
data to insights in just a few clicks — no prior experience in Kibana required.


Task: What we would like to understand is the health of the webapp: how many requests were returning 2** vs 5**

Without Lens:
![Alt text](images/status_code_chart.png?raw=true)

![Alt text](images/status_code_chart_config.png?raw=true)

With Lens:
![Alt text](images/lens.png?raw=true)

Could you also answer the following questions with line, bar or pie charts:
* where do most of the requests coming from over the last 7 days? Location wise and Ip wise?
* what is the percentage of the error logs?
* what are the top requests?
* what extension is used the most?
* what browser do most customers use?


## Homework
1. We would always wanna be notified if the system goes wrong, what does an alert mean?

   * The configuration is not set for this system. Could you do your own research and see how to set up an alert for the logging system?
e.g. alert when the number of 5xx > 10

2. Is it possible to set up dashboards with terraform?