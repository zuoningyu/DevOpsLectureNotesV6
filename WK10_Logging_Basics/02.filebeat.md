# Integrate the ELK with filebeat

![Alt text](images/high_level.png?raw=true)
reference: https://www.freecodecamp.org/news/docker-container-log-analysis-with-elastic-stack-53d5ec9e5953/

## Step 1. Let us fix something in docker-elk repo
Go to the docker-elk folder that you cloned (mentioned in 01.elk_hands_on.md)
```
cd docker-elk
```
Open logstash -> pipeline -> logstash.conf and add 
```
index => "%{[@metadata][beat]}-%{[@metadata][version]}"
```
on line 18 
```
...
output {
	elasticsearch {
		hosts => "elasticsearch:9200"
		user => "elastic"
		password => "changeme"
		index => "%{[@metadata][beat]}-%{[@metadata][version]}"
	}
}
```
This is to index the logs that are sent from filebeats
## Step 2. Rebuild and run the docker-compose
```
docker-compose build
docker-compose up
```
In another window, type 
```
docker-compose ps
```
You should see
```
           Name                         Command               State                                               Ports                                             
--------------------------------------------------------------------------------------------------------------------------------------------------------------------
docker-elk_elasticsearch_1   /bin/tini -- /usr/local/bi ...   Up      0.0.0.0:9200->9200/tcp, 0.0.0.0:9300->9300/tcp                                                
docker-elk_kibana_1          /bin/tini -- /usr/local/bi ...   Up      0.0.0.0:5601->5601/tcp                                                                        
docker-elk_logstash_1        /usr/local/bin/docker-entr ...   Up      0.0.0.0:5000->5000/tcp, 0.0.0.0:5000->5000/udp, 0.0.0.0:5044->5044/tcp, 0.0.0.0:9600->9600/tcp

```



## Step 3. Let us build the filebeat docker container
```
cd filebeat
docker build -t fcc/filebeat .
```
Build the docker image

## Step 4. Run the filebeat agent to collect docker logs
Run filebeat with the following command to collect docker logs 

```
docker run --rm -v '/var/lib/docker/containers:/usr/share/dockerlogs/data:ro' -v '/var/run/docker.sock:/var/run/docker.sock' --name filebeat fcc/filebeat:latest
```

In the above Docker command, note the two bind mount parameters: /var/lib/docker/containers is the path where docker 
logs exist within the host machine, and it has been bound to /usr/share/dockerlogs/data path within Filebeat container
with read only access.

In the second bind mount argument, /var/run/docker.sock is bound into the Filebeat containerâ€™s Docker daemon. 
It is the unix socket the Docker daemon listens on by default and it can be used to communicate with the daemon 
from within a container. This allows our Filebeat container to obtain Docker metadata and enrich the container log 
entries along with the metadata and push it to ELK stack.

## Step 5. login to kibana and try to create the index
Login to Kibana and Go to `Stack Management` -> `Kibana` -> `Data View` -> `Create`

![Alt text](images/create_data_view.png?raw=true)

You will not see `filebeat-7.2.0` index patterns but some generic ones only.

This is because we have errors in our logs

```
logstash_1       | [2022-06-27T11:42:29,498][INFO ][logstash.outputs.elasticsearch][main]
[495aaf9432a30cd7a341bf5aa2516b8e676e8f0b36e9f332d23786beb5574df2] Retrying failed action
 {:status=>403, :action=>["index", {:_id=>nil, :_index=>"filebeat-7.2.0", :routing=>nil}, 
 {"container"=>{"id"=>"3a070ddebde91dece2a008789f27d958bd851b674942efe850c09a1398fe33a2"}, 
 "stream"=>"stderr", "event"=>{"original"=>"W0625 07:31:11.091697       1 watcher.go:229] 
 watch chan error: etcdserver: mvcc: required revision has been compacted"}, 
 "tags"=>["beats_input_codec_plain_applied"], 
 "@version"=>"1", "host"=>{"name"=>"b86a955ef061"}, 
 "input"=>{"type"=>"docker"}, 
 "@timestamp"=>2022-06-25T07:31:11.094Z, 
 "message"=>"W0625 07:31:11.091697       1 watcher.go:229]
  watch chan error: etcdserver: mvcc: required revision has been compacted", 
  "agent"=>{"type"=>"filebeat", "ephemeral_id"=>"0d7f7643-59d2-40fc-b3c5-4174a9af49e9", 
  "hostname"=>"b86a955ef061", "version"=>"7.2.0", "id"=>"387bd947-1740-44fa-aed8-e607f5402b08"}, 
  "ecs"=>{"version"=>"1.0.0"}, "log"=>{"offset"=>168212, 
  "file"=>{"path"=>"/usr/share/dockerlogs/data/3a070ddebde91dece2a008789f27d958bd851b674942efe850c09a1398fe33a2/3a070ddebde91dece2a008789f27d958bd851b674942efe850c09a1398fe33a2-json.log"}}}], 
  :error=>{"type"=>"security_exception", 
                   "reason"=>"action [indices:admin/auto_create] is unauthorized for user [logstash_internal] with roles
                              [logstash_writer] on indices [filebeat-7.2.0], this action is granted by the index 
                              privileges [auto_configure,create_index,manage,all]"}}
```
Search `is unauthorized for user [logstash_internal] with roles
[logstash_writer]`

You will find this thread https://github.com/deviantony/docker-elk/issues/687

## Step 6. Fix the security issue

For simplicity, we take the option to give logstash_internal a superuser privilege

![Alt text](images/superuser.png?raw=true)

After updating, you should see no more errors. 


## Step 7. Create the data view/index pattern

go to `stack management` from your sidebar

![Alt text](images/create_data_view.png?raw=true)

type `filebeat-*` and select `@timestamp`

click "Create data view"

## Step 6. Viola! Let us check the docker logs
You should see the following in `http://localhost:5601/app/discover` 
![Alt text](images/docker_logs.png?raw=true)


## Homework
Filebeat has been evolved and can replace logstash to some extent.
![Alt text](images/evo.png?raw=true)

Could you follow the guide and try this different setup? 
https://medium.com/@bcoste/powerful-logging-with-docker-filebeat-and-elasticsearch-8ad021aecd87

Once you have done the above practice, you should be able to set up logging for EasyCRM. Try it out and see how you go. 