# Open question
* What is logging? 
* What log types do we know?
* If you are a developer, what do you use the logs for?
* Which tool do you use for logging?
* How do you tell if your application is buggy or not by log?
* How frequent do you collect the logs?
* What if you have millions of logs, how do you find the useful one?



# Logging basics
- Visibility is key for your application

## What is logging?
In computing, a log file is a file that records either events that occur in an operating system or other software runs,
 or messages between different users of a communication software. 
 Logging is the act of keeping a log. In the simplest case, messages are written to a single log file.

```
- Logging
   - Event Logs
   - Transaction Logs
```

### Event logs 
Event logs record events taking place in the execution of a system in order to provide an __audit__ trail that can be used 
to understand the activity of the system and to diagnose problems. They are essential to understand the activities of 
complex systems or applications.

Keywords: record, events, audit, activities of the system or application

### Transaction logs
 
Most database systems maintain some kind of transaction log, which are not mainly intended as an audit trail for later 
analysis, and are not intended to be human-readable. These logs record changes to the stored data to allow the database 
to recover from crashes or other data errors and maintain the stored data in a consistent state. Thus, database systems 
usually have both general event logs and transaction logs

Keywords: transaction, database, human-unreadable

Today, we focus mainly on event logs. We have talked about to many concepts. Let us look at the big picture.
What do we use the logs for and what does the logs look like?


# Why do we log?
## Logging Purpose
Debugging/Incident Analysis/Optimisation
  * err stack
  * src/dst/request rate
  * http info
  * request info
  * thread name
  * process id
  * avoid cross region requests 
 
Incident Detection
  * request rate/error rate
  * error threshold
  * capture NPE/runtime exception automatically

   
Health monitoring
  * request counts
  * request rate/error rate
  * request duration
  * request size
  * request method
  * healthcheck status
  
Auditing
  * time
  * user id
  * action name 
  * method
  
Rollback
  * transaction log only
 
  
# What do we log?
## System Logs vs Application Logs
```
* System Logs 
    * Linux/Unix/Windows/Docker Logs
* Application Logs
    * Access Logs (e.g. Apache Logs/Nginx Logs)
    * Web Server Logs (e.g. Python Logfile, Log4j)
    * Cache Logs (May Contain PII or UGC)
    * Database Logs (May Contain PII or UGC)
    * Message Queue Server Logs (e.g. Python Logfile, Log4j)
```

## System Logs

### Generic system activity logs
```
cat /var/log/messages # Ubuntu
```
or 
```
cat /var/log/syslog # Debian
```
* informational 
* non-critical system messages.

### Authentication events
```
cat /var/log/auth.log # Ubuntu/Debian
```
or 
```
cat /var/log/secure # CentOS
```
* authentication
* authorisation

### Initialisation Logs
```
cat /var/log/boot.log
```
* The system initialization script

### Device/driver messages
* Kernel ring buffer messages
* Hardware devices and their drivers

#Application Logs
### Apache server logs
```
cat var/log/httpd/
```
* logs recorded by the Apache server.
* error log and access log.

Error Log - the web server encountered when processing requests
```
[Thu Mar 13 19:04:13 2014] [error] [client 50.0.134.125] File does not exist: /var/www/favicon.ico
```

Access Log - requests coming in to the web server
```
10.185.248.71 - - [09/Jan/2015:19:12:06 +0000] 808840 "GET /inventoryService/inventory/purchaseItem?userId=20253471&itemId=23434300 HTTP/1.1" 500 17 "-" "Apache-HttpClient/4.2.6 (java 1.5)"
```


### Access Log
- HTTP Request passing through the load balancer/apache server
    * Time
    * Status Code
    * Url Path
    * Remote IP or Host
    * Request Time Seconds
    * Request ID
    * User Agent
    * Region Info
    * Thread Name
    * Process Id
    * Trace Id
    
### Application Log    
- HTTP Request passing through the application
    * Server info (EC2 ID, IP, Version etc..)
    * Deployment info (Deployment Id or Tag)
    * Log Level
    * Logger Name (Class)
    * Error Message
    * Request ID
    * Url Path
    * Region Info
    * (Optional) Tenant Url
    * (Optional) Shard Info
    



# How do we log?
Modern logging system contains three parts:

1) Logging Aggregator/Forwarder
  * Collect/Aggregate logs
  * Forward Logs
  * Process the log format

2) Search 
  * Indexing by default/customisation

3) Visualisation 
  * UI Search bar
  * Timeline
  * Pie/Bar/Other Charts

![Alt text](images/technolush-elk-stack.png?raw=true)

The most common ones: 
* ELK stack- FILEBEAT + Logstash + Elastic Search + Kibana
* Splunk stack - Fluentd + Splunk


## Log Aggregation
https://sematext.com/blog/log-aggregation/
* File Replication
  - copy your log files to a central location such as rsync and cron. 
  
    However, although it does bring together all of your logs, this option is not really the same as an aggregation, 
    but more of a “co-location.” 
    
    Furthermore, since you need to follow a cron schedule, in the long-term, file replication is not a good solution
    as you don’t get real-time access to your log data.
    
* Forwarder(e.g. Syslog) 
  - They allow processes to send log entries to them that they’ll then redirect to a central location.
    
    You need to set up a central syslog daemon on your network as well as the clients. 
    The client logging daemons will forward these messages to the daemons.
                                                                                     
    Syslog is also a simple method to aggregate your logs since you have already installed it and you
    only have to configure it. The catch is to make sure the central syslog server is available and 
    figure out how to scale it.
    https://en.wikipedia.org/wiki/Syslog
    
    
## Example:  Using Logstash to collect/aggregate logs    
![Alt text](images/logstash-instance-input-filter-output-plugins.png?raw=true)

Logstash is a plugin-based data collection and processing engine. It comes with a wide range of plugins that makes it 
possible to easily configre it to collect, process and forward data in many different architectures.

Processing is organized into one or more pipelines. In each pipeline, one or more input plugins 
https://www.elastic.co/guide/en/logstash/current/input-plugins.html receive or collect data
that is then placed on an internal queue. This is by default small and held in memory, but can be configured to be 
larger and persisted https://www.elastic.co/guide/en/logstash/6.2/persistent-queues.html on disk in order to improve
 reliability and resiliency.
 
### What is the persistent queue for? How does it improve the reliability or resiliency?
There are chances that logstash can fail/crash with errors such as filter errors or cluster unavailability. This can
lead to data loss in the log monitoring system. To guard against such data loss, Logstash provides data resilience 
mechanisms such as persistent queues and dead letter queues.

### Where is the log data kept?
Answer: Disk

In the default configuration, Logstash keeps the log data in in-memory queues. The size of these in-memory queues is 
fixed and not configurable. As the primary memory is volatile, in case of machine failures, the data in the memory 
queue will be lost. If we enable the persistent queue mechanism in Logstash, the message queue will be stored in 
the disk. Enabling this feature helps in removing additional architectural modification by adding some buffering layer 
before Logstash pipelines. Also, this assures log delivery even in the case of system shutdown, restart, or temporary 
failure.


Persistent queue works in between the input and filter section of Logstash. To configure persistent queue-enabled 
Logstash, we need to update the logstash.yml.

```
queue.type: persisted
path.queue: "path/to/data/queue" 
queue.max_bytes: 4gb
```

In addition to persistent queue, we can use dead letter queue to store write failed events, process them, 
and place them back in the Elasticsearch index. This feature works only in the Elasticsearch output option. 
For configuring this change, we need to add the following configuration settings in the logstash.yml file. 
Also, we can define the size of the dead letter queue by setting dead_letter_queue.max_bytes. The default value is 1gb.
 Logstash has the dead_letter_queue input plugin to handle the dead letter queue pipeline.
 
```
dead_letter_queue.enable: true
path.dead_letter_queue: "path/to/data/dead_letter_queue"
dead_letter_queue.max_bytes: 2gb
```

### What if the server memory is not big enough?

We can ensure data resilience by using Logstash persistent queuing mechanism in small environments with a low volume of
logs. But in the case of log monitoring systems that handle a large volume of logs, we need to add some message
queueing mechanism in between Beats and Logstash. Three commonly used tools for message queueing are Apache Kafka, 
Redis, and RabbitMQ.

![Alt text](images/Elastic-Stack-architecture-with-buffering-layer.png?raw=true)

#####Extra reading (https://blog.qburst.com/2020/01/a-deep-dive-into-log-monitoring-using-elastic-stack/)
* Apache Kafka: Apache Kafka is a distributed streaming platform that can publish and subscribe to streams of records. 
The components that generate streams (here logs) and send them to Kafka are the publishers (here it is Beats) and the
components that pull logs from Kafka are the subscribers (here it is Logstash). Kafka stores data in different topics.
Each topic has a unique name across the Kafka cluster. The publisher can write messages to Kafka-topics and consumers
can consume data from the Kafka-topics.

* Redis: Redis is an open-source, in-memory data store that can be used as a message broker. 
It stores data as key-value pairs. Redis is really fast as it uses in-memory data store. 
On the other hand, it becomes a weakness when memory is full. Log data will be dropped when the Redis queue is full.
Redis also offers disk-based data persistence to avoid data loss, but the performance will be reduced due to disk
latency.

* RabbitMQ: RabbitMQ is an open-source enterprise message queueing system based on Advanced Message Queueing Protocol
(AMQP) written in Erlang. In RabbitMQ, we have brokers, producers, and consumers. Brokers are responsible for tracking
which messages are delivered to which consumers. RabbitMQ is a good choice when you have less than thousands of logs
per second.

### What is filters/outputs?
Processing threads read data from the queue in micro-batches and process these through any configured filter plugins
in sequence. Logstash out-of-the-box comes with a large number of plugins targeting specific types of processing, and
this is how data is parsed, processed and enriched.
 
Once the data has been processed, the processing threads send the data to the appropriate output plugins, which
are responsible for formatting and sending data onwards, e.g. to Elasticsearch.
 
Input and output plugins can also have a codec plugin configured. This allows parsing and/or formatting of data
before it is put onto the internal queue or sent to an output plugin.


#### Filters
Filters are intermediary processing devices in the Logstash pipeline. You can combine filters with conditionals to perform an action on an event if it meets certain criteria. Some useful filters include:

* grok: parse and structure arbitrary text. Grok is currently the best way in Logstash to parse unstructured log data into something structured and queryable. With 120 patterns built-in to Logstash, it’s more than likely you’ll find one that meets your needs!
mutate: perform general transformations on event fields. You can rename, remove, replace, and modify fields in your events.
* drop: drop an event completely, for example, debug events.
* clone: make a copy of an event, possibly adding or removing fields.
* geoip: add information about geographical location of IP addresses (also displays amazing charts in Kibana!)
For more information about the available filters, see Filter Plugins https://www.elastic.co/guide/en/logstash/6.2/filter-plugins.html.

#### Outputs

Outputs are the final phase of the Logstash pipeline. An event can pass through multiple outputs, but once all output
processing is complete, the event has finished its execution. Some commonly used outputs include:

* elasticsearch: send event data to Elasticsearch. If you’re planning to save your data in an efficient, convenient, and easily queryable format…​Elasticsearch is the way to go. Period. Yes, we’re biased :)
* file: write event data to a file on disk.
* graphite: send event data to graphite, a popular open source tool for storing and graphing metrics. http://graphite.readthedocs.io/en/latest/
* statsd: send event data to statsd, a service that "listens for statistics, like counters and timers, sent over UDP and sends aggregates to one or more pluggable backend services". If you’re already using statsd, this could be useful for you!
For more information about the available outputs, see Output Plugins.


Let us look at some examples in "parsing the logs", "filters" and "transforming logs"
https://www.tutorialspoint.com/logstash/logstash_parsing_the_logs.htm

Extra Reading: 
https://www.elastic.co/guide/en/logstash/current/config-examples.html
https://sematext.com/blog/getting-started-with-logstash/
