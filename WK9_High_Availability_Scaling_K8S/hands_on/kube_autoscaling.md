# Kubernetes Autoscaling
This is the handson to show you use kubectl tools to autoscale the application.

# Prerequisite 

## Kind and Kubectl
- kind is a tool for running local Kubernetes clusters using Docker container “nodes”.
https://kind.sigs.k8s.io/
- use kubectl to interact with it by using the configuration file generated by kind.

Please install kind, kubectl, and go if you haven't

* https://kind.sigs.k8s.io/docs/user/quick-start/
* https://kubernetes.io/docs/tasks/tools/install-kubectl/
* https://golang.org/doc/install

## create the cluster
```
kind create cluster
```
## verify the cluster

```
kind get clusters
```

Note:
- if you check `docker ps`, you should be able to see a node kind container running.
- Creating cluster will bootstrap a Kubernetes cluster using a pre-built node image - you can find it on docker hub kindest/node. If you desire to build the node image yourself see the building image section. To specify another image use the --image flag.
- By default, the cluster will be given the name kind. Use the --name flag to assign the cluster a different context name.


```
kubectl cluster-info
```

You should be able to see

![Alt text](../images/kind-kubectl.png?raw=true)

# Autoscaling

## Explore the example App
A simple distributed application running across multiple Docker containers.

https://github.com/dockersamples/example-voting-app

![Alt text](../images/app-localhost.png?raw=true)

![Alt text](../images/app-result.png?raw=true)

## Setup
```
git clone https://github.com/dockersamples/example-voting-app
```

###Step 1. Go to the cloned folder and create namespace
```
cd example-voting-app
kubectl create namespace vote                                                           
```
You should see
```
namespace/vote created
```
###Step 2. Remove k8s-specifications/vote-namespace.yml
```
rm k8s-specifications/vote-namespace.yml
```
Modify k8s-specifications/vote-deployment.yml
changed: 
```
spec:
  replicas: 1
```
to 
```
spec:
  replicas: 3
```

Note:
- Namespaces provide a scope for names. Names of resources need to be unique within a namespace.

###Step 3. Now let us create the cluster
```
kubectl create -f k8s-specifications/                                                   
```
It should return
```
deployment.apps/db created
service/db created
deployment.apps/redis created
service/redis created
deployment.apps/result created
service/result created
deployment.apps/vote created
service/vote created
deployment.apps/worker created
```

###Step 4. Check the status
```
kubectl get pods --namespace=vote --output=wide
kubectl get services --namespace=vote
```

###Step 5. Port Forward
Since we are not on google cloud and don't have a loadbalancer setup, we can port forward to access our app:

Use a new terminal window and type the following:
```
kubectl port-forward svc/vote --namespace=vote 5000:5000
```

You should be able to visit http://localhost:5000, which is the vote service.

![Alt text](../images/app-localhost.png?raw=true)

Use a new terminal window and type the following:
```
kubectl port-forward svc/result --namespace=vote 5001:5001
```

You should be able to visit http://localhost:5001, which is the result service.

![Alt text](../images/app-result.png?raw=true)

Also, try to understand what does these mean:
```
kubectl get pods --namespace=vote --output=wide
kubectl get svc --namespace=vote
kubectl get deploy -n vote

```

Note:
- `--namespace` can be short as `-n`
- there are short names there. For example, pod (po), service (svc), replicationcontroller (rc), deployment (deploy), replicaset (rs) , etc.

Try to delete a pod and see if it gets deleted:

Do you know how to find out the right command to use?

- Q: What do you see and why? So which component controls the pods?
- Q: Will the corresponding service still exist?

Note:
- You can also try to see the log, which is useful if something is wrong, e.g.
```
kubectl logs svc/vote -n vote
```


## Manual Scaling
You can set the replicas to higher number and it should generate more nodes for each deployment.
You can also scale the nodes manually. Are you able to find the command?

### Step 1. check the current replicas
```bash
kubectl get rs -n vote
```
### Step 2. manually scale the replica for vote deployment
```bash
kubectl scale --replicas=6 deployment/vote -n vote
```
### Step 3. check the replica for the deployment and also check the vote detail
```
kubectl get deploy -n vote
```
```
kubectl describe deploy/vote -n vote
```
so what is the big deal of setting replica?

## Auto Scaling
### Step 1. Let us set the Auto Scaling Policy for our nodes
```
kubectl autoscale deployment vote --namespace=vote --cpu-percent=50 --min=1 --max=10
```
### Step 2. Check the settings:
```
kubectl get hpa --namespace=vote
```

### Step 3. Debug
Let us check what is wrong?
Did you see `<unknown>/50%`?
```
kubectl describe hpa --namespace=vote
```
We will see the error and warning message as below.

![Alt text](../images/horizonal-pod-autoscaler-error.png?raw=true)

To fix it, we need to spin up metric server. 
1. Visit https://github.com/kubernetes-sigs/metrics-server
2. Look at Deployment section
3. Inspect `components.yaml` file under the hands_on folder. The highlight is as below.
```
containers:
- name: metrics-server
    image: k8s.gcr.io/metrics-server-amd64:v0.3.1
    ## add the config as below
    command:
      - /metrics-server
      - --kubelet-insecure-tls
      - --kubelet-preferred-address-types=InternalIP
    ## add the config as above  
```
4 . Apply the resource
```
kubectl apply -f ./hands_on/components.yaml
```

5 . Check Metric Server status

```
kubectl get pods -n kube-system
kubectl logs <pod> -n kube-system 
```
You should be able to see metric server running

6 . Go to example-voting-app repo and Delete the old hpa:

```
kubectl delete hpa --all --namespace=vote
```

7 . Stop the current app:
```
kubectl delete -f k8s-specifications/
``` 
8 . Add the following to `vote-deployment.yaml` under container
```
        resources:
          limits:
            cpu: 200m
          requests:
            cpu: 100m
```

as like
```
    spec:
      containers:
      - image: dockersamples/examplevotingapp_vote:before
        name: vote
        ports:
        - containerPort: 80
          name: vote
        resources:
          limits:
            cpu: 200m
          requests:
            cpu: 100m
```

See more https://kubernetes.io/docs/tasks/configure-pod-container/assign-cpu-resource/

9 . Recreate everything:
```
kubectl create namespace vote
kubectl create -f k8s-specifications/
``` 
10 . Recreate asg rule:
```
kubectl autoscale deployment vote --namespace=vote --cpu-percent=50 --min=1 --max=10
```
In the beginning, you will still see the metric error, but **after a couple of mins**, then you should see
```
kubectl get hpa --namespace=vote                                                        
NAME   REFERENCE         TARGETS   MINPODS   MAXPODS   REPLICAS   AGE
vote   Deployment/vote   xxx%/50%     1         10        1          38s
```

You can check what happen with
```
kubectl describe hpa -n vote
```

- Q: Do you know why it is like that?

11 . Test scale

We can give a low cpu-percent so it will scale up. 
```
kubectl autoscale deployment vote --namespace=vote --cpu-percent=1 --min=1 --max=10
```

Or more interestingly, we can increase the load.
In the real world, so we will see how the autoscaler reacts to the increased load.

forward the port first
```
kubectl port-forward svc/vote --namespace=vote 5000:5000
```

We will send an infinite loop of queries to localhost:5000. Note you should keep the port forwarding work in the example-voting-app.

run it in a different terminal:
```
./generate-load.sh
```

After a minute or so, you can stop the load script, and check the hpa again.

Note: If the load is lower, then it can also scale down.
```
kubectl describe hpa --namespace=vote
```
You will find it automatically scale up and down

![Alt text](../images/scale-up-down.png?raw=true)

```
kubectl get po -n vote
```
You will find more pods for vote

![Alt text](../images/auto-scaling-pod2.png?raw=true)

It is required to balance the auto-scaling policy with the resource limit.

See more https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/
https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/

## Something about Resource limit.

- Q: Explain CPU units, what`cpu: XXXm` mean?
    The CPU resource is measured in CPU units. One CPU, in Kubernetes, is equivalent to:
    
    - AWS vCPU
    - GCP Core
    - Azure vCore
    - Hyperthread on a bare-metal Intel processor with Hyperthreading
    
    Fractional values are allowed. A Container that requests 0.5 CPU is guaranteed half as much CPU as a Container that requests 1 CPU. You can use the suffix m to mean milli. For example 100m CPU, 100 milliCPU, and 0.1 CPU are all the same. Precision finer than 1m is not allowed.
    
    CPU is always requested as an absolute quantity, never as a relative quantity; 0.1 is the same amount of CPU on a single-core, dual-core, or 48-core machine.
    
- Q: What if you specify a CPU request that is too big for your Nodes,

    Try to edit
    ```
        resources:
          limits:
            cpu: "100"
          requests:
            cpu: "100"
    ```
    Stop/restart again
    
    ```
    kubectl get pods -n vote
    NAME                      READY   STATUS             RESTARTS   AGE
    db-6789fcc76c-bb8hc       1/1     Running            0          21s
    redis-554668f9bf-qzv58    1/1     Running            0          21s
    result-79bf6bc748-cxwf4   1/1     Running            0          21s
    vote-bfb5bbc9d-m4kfr      0/1     Pending            0          21s
    worker-dd46d7584-gcbbk    0/1     CrashLoopBackOff   1          21s
    ```
    The output shows that the Pod status for vote is Pending. That is, the Pod has not been scheduled to run on any Node, and it will remain in the Pending state indefinitely
    
    ```
    kubectl describe pod vote-bfb5bbc9d-m4kfr -n vote
    ```
    
    then you will see error as below
    ```
    Events:
      Type     Reason            Age                  From               Message
      ----     ------            ----                 ----               -------
      Warning  FailedScheduling  14s (x5 over 3m17s)  default-scheduler  0/1 nodes are available: 1 Insufficient cpu.
    ```

- Q: What if you do not specify a CPU limit?

    If you do not specify a CPU limit for a Container, then one of these situations applies:

    1. The Container has no upper bound on the CPU resources it can use. The Container could use all of the CPU resources available on the Node where it is running.
    
    2. The Container is running in a namespace that has a default CPU limit, and the Container is automatically assigned the default limit. Cluster administrators can use a LimitRange to specify a default value for the CPU limit.

- Motivation for CPU requests and limits 

    By configuring the CPU requests and limits of the Containers that run in your cluster, you can make efficient use of the CPU resources available on your cluster Nodes. 
    
    By keeping a Pod CPU request low, you give the Pod a good chance of being scheduled. By having a CPU limit that is greater than the CPU request, you accomplish two things:

    1. The Pod can have bursts of activity where it makes use of CPU resources that happen to be available.
    
    2. The amount of CPU resources a Pod can use during a burst is limited to some reasonable amount.

see more https://kubernetes.io/docs/tasks/configure-pod-container/assign-cpu-resource/


## Further Reading and References
https://www.stackrox.com/post/2020/05/kubernetes-autoscaling-explained/

